%!TEX root=betamain.tex


\section{Related Work}
\label{sec:related}

%\agp{Say something about the three categories that you list below.}

In this section, we first introduce existing studies on annotation bias of crowds, 
when data items are presented either independently, or in a sequence or batches; 
we then introduce rank aggregation techniques and their application on crowdsourced ranking or rating.  

%\agp{I fixed all the past tense references below. If I missed something, you should fix.}

% \agp{In some cases, it is not sufficient to say that the papers are not on the same topic,
% we need to argue that the techniques are different or the models are different. Take another pass on this section to make sure we say {\em something} about every set of papers.}

\vpara{Annotation bias in independent judgments.}
A number of studies have been conducted on verifying and quantifying annotation bias of crowd workers.  
%However, these studies mainly consider the scenario when data items are judged independently.  
Snow~\etal~\cite{snow:emnlp2008} explore the performance of annotations by non-expert workers for several NLP tasks.
Demeester~\etal~\cite{demeester:wsdm2014} discuss the disagreement between different users on assessment of web search results.

There are also extensive studies on modeling worker behaviors.  
Raykar~\etal~\cite{raykar:nips2011ranking,raykar:icml2009,raykar:jmlr2010}
study how to learn a model with noisy labeling. 
Specifically, they employ a logistic regression classifier, 
and insert hidden variables indicating whether a worker tells the truth. 
%They utilize an EM algorithm to infer the real ground-truth while training the model.
Karger~\etal~\cite{karger:nips2011} propose an iterative algorithm to infer workers' reliability 
and aggregating their answers.  
Whitehill~\etal~\cite{whitehill:nips2009} model the annotator ability, data item difficulty, and infer the true label from the crowds in a unified model.
Most of these work also proposes various generative model to capture worker behavior.  
However, they assume judgments on different data items are independent, 
which is not necessarily true when data items are grouped into batches.  

Venanzi~\etal~\cite{venanzi:www2014} propose a community-based label aggregation model to identify different types of workers, 
and correct their labels correspondingly.  
Das~\etal~\cite{das:kdd2013} address the interactions of opinions between people connected by networks.
They focus on another aspect of dependencies, 
which is the dependencies between workers, 
while in our studies, we are more concerned about dependencies between data items and their judgments.  

% \agp{Say something like: Like this work, many of these papers
% also consider EM-based techniques; however, to the best of our knowledge,
% we are the first to articulate and provide solutions for the problem
% of debiasing of batches -- either at the start or the end of this portion.}


\vpara{Annotation bias in sequential and batch judgments.}
A few researchers also notice the correlation between judgments on different data items, 
but their work are mainly developed in the setting when data items are reviewed in a sequence.  
Scholer~\etal~\cite{scholer:sigir2013,scholer:sigir2011} study the annotation disagreements in 
a relevance assessment data set.  
%TREC data set, which is on relevance assessment tasks in information retrieval.  
They discover correlations between annotations of similar data items.  %, and the estimated time between the two annotation.  
They also explore ``threshold priming'' in annotation, 
where the annotators tend to make similar judgments or apply similar standard on consecutive data items they review. 
%tend to make similar judgments to consecutive data items.  
However, their work focuses on the scenario when data items are organized in a long sequence.  
It confines the dependencies to exist only between consecutive data items.  
Also, they focus more on qualitative conclusions, without a quantitative model to characterize and measure the discovered factors.  
Carterette~\etal~\cite{carterette:effect2010} provide several assessor models for the TREC data set.  
Mozer~\etal~\cite{mozer:nips2010} study the similar ``relativity of
judgments'' phenomenon on sequential tasks instead of batches.  
Again, their focus is more on data items presented as a long sequence, 
while we focus more on data items presented in batches simultaneously.  

Our recent work~\cite{zhuang:wsdm2015} also considers a similar setting 
when data items are organized in batches; 
we verify the existence of annotation bias caused by batching data items.  
Our focus in that paper was to design an active learning algorithm to smartly assemble batches, 
aiming to improve the performance of the classifier trained on this annotation batches.  
Our focus was not on improving the quality of labels collected, 
and we still used majority voting to obtain labels for data items.  
In this paper, we focus on debiasing the obtained labels directly,
the higher label quality can benefit tasks including training and/or evaluating classifiers, 
with a broader range of applications.

\vpara{Crowdsourced ranking and rating.}
In our model, we employ the Plackett-Luce model to capture worker behavior, 
and aggregate worker annotations on batches as rankings in order to infer true labels.  
There is a related thread of work on rank aggregation; 
however, to the best of our knowledge, 
we are the first to model crowds' annotating behavior on batches by ranking, 
and propose a debiasing strategy. 

Studies on aggregating multiple rankings into a consistent ranking can be dated back to the seminal work of Arrow~\cite{arrow2012social}.  
Negahban~\etal~\cite{negahban:nips2012} study how to aggregate pairwise comparisons into a ranking 
by utilizing the Bradley-Terry model~\cite{bradley:1952}, which is a simplified version of Plackett-Luce model utilized in this paper.   
Hunter~\etal~\cite{hunter:aos2004} propose the minorization-maximization (MM) algorithm to infer Plackett-Luce model 
from multiple partial orderings.  
Soufiani~\etal~\cite{soufiani:nips2013} generalize Negahban~\etal's work 
and proposed a class of generalized method-of-moments (GMM) algorithm 
to infer parameters of Plackett-Luce model from multiple orderings, 
and compare the performance against MM-algorithm.  
They then further extend their algorithm 
to be applied to a more general class of ranking models called random utility models (RUMs)~\cite{soufiani:icml2014}.  
In addition, the technique for rank aggregation has also been studied 
in context of information retrieval~\cite{dwork:www2001,klementiev:icml2008,liu:www2007,qin:nips2010,volkovs:www2012}.  
%A summary of early work can be found in~\cite{dwork:www2001}.
These studies do not explicitly address the crowdsourcing settings 
to actually model the worker behavior.  
Directly applying their techniques (\eg~\cite{hunter:aos2004}) may not lead to better performance, 
as shown in our experiments.  

There is related research on aggregating multiple rankings or leveraging crowds' power to obtain ranking of data items.  
Chen~\etal~\cite{chen:wsdm2013} study aggregating crowdsourced annotation on pairwise comparison to obtain a ranking on data items. 
Mao~\etal~\cite{mao:aaai2013} show how aggregated results of noisy voting obtained from crowdsourcing platform 
may differ by using different aggregating strategies.  
However, their objective is just to obtain a ranking, 
while our model incorporates a ranking model but the ultimate goal is still to collect labels for data items.  

%They employed the simplified version of Plackett-Luce model for pairwise comparison, Bradley-Terry model~\cite{bradley:1952}, 
%and extended the model to adapt the crowdsourcing settings.  
%Similar problem is also studied by Volkovs~\etal~\cite{volkovs:www2012}, 
%while they proposed a multinomial preference model for partial ranking of data items.  

%and apply the model on aggregating crowdsourced comparison on data item pairs into a ranking.  
%Furthermore, our model can deal with not only pairwise comparisons, 
%but also the situation with more than two data items in a batch.  

Several papers also consider crowdsourced rating.  
Parameswaran~\etal~\cite{parameswaran:vldb2014} focused on crowdsourced rating on items, 
and applied their system on a peer evaluation data set of a MOOC course.  
Crowdsourcing has also been utilized for rating multimedia content quality~\cite{chen:mm2009} and relevance assessment~\cite{alonso:2008}.  
However, they do not explicitly study the scenario when data items are grouped into batches.  

\hide{
\vpara{Batch active learning.}
Active learning has been extensively studied.
Settles~\etal~\cite{settles:2010survey} summarized a number of active learning strategies.
Batch active learning, in contrast to traditional active learning settings, aims to choose a set of data items to query, which proposes some unique challenges.
Some strategies focus on optimizing the helpfulness of a data batch.
Hoi~\etal~\cite{hoi:icml2006,hoi:www2006} utilized Fisher information matrix to choose the data items that are likely to change the classifier parameters most.
Brinker~\etal~\cite{brinker:icml2003} studied batch active learning for SVM and aimed to maximize the diversity within the selected set of data samples.
Guo~\etal~\cite{guo:nips2008} proposed discriminative active learning strategy by formulating the problem as an optimization problem.
% Azimi~\etal~\cite{azimi:icml2012} propose a Monte-Carlo method to sequentially select the batch of samples to query.
A number of strategies also aim at choosing the most representative data batch with regard to the unlabeled data set.
Yu~\etal~\cite{yu:icml2006} proposed a transductive experimental design, which prioritizes the data samples that represent the hard-to-predict data.
Chattopadhyay et al.~\cite{chattopadhyay:kdd2012} tried to choose the data batch to minimize Maximum Mean Discrepancy (MMD) to measure the difference in distribution between the labeled and unlabeled data.
There are studies addressing both intuitions.
Wang~\etal~\cite{wang:kdd2013} designed a framework to minimize the upper bound of empirical risk, which aims to find a batch of data items that are both discriminative and representative.
However, all of the above studies assume reliable oracles---which never are in multiple-annotator scenarios such as crowdsourcing.

\vpara{Active learning with crowds.}
Crowdsourcing serves as a potentially ideal oracle for active learning.
Two perspectives have been explored; 
first is how to select data items to query when the oracles are noisy.
Sheng~\etal~\cite{sheng:kdd2008} provided an empirical study on the performance of repeated labeling and developed an active learning strategy addressing both the ``label uncertainty'' and ``model uncertainty''.
Second is how to select annotators for crowdsourcing.
Donmez~\etal~\cite{donmez:kdd2009} studied this problem, by modeling the querying problem as a multi-armed bandit problem.  Each annotator is regarded as as a bandit, and a binary reward function is defined based on whether the oracle provides a correct label.  %However, the true label is unknown. They hence utilize majority voting to determine the ground truth.
Yan~\etal~\cite{yan:icml2011} explore both the problem of selecting query samples and selecting oracles, in context of a logistic regression classifier.
Kajino~\etal~\cite{kajino:aaai2012} proposed a convex optimization function for active learning in crowds.  
% vijayanarasimhan~\etal~\cite{vijayanarasimhan:cvpr2011} applied crowdsourcing to actively learn the detector for p
However, none of them leverages in-batch bias for active learning.
}


