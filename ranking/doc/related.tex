\section{Related Work}
\label{sec:related}

\vpara{Annotation bias in independent judgments.}
A number of studies have been conducted on evaluating data quality collected from crowds, 
and modeling worker behaviors to optimize the data quality.  

However, these studies mainly consider the scenario when data items are judged independently.  
Snow~\etal~\cite{snow:emnlp2008} explored the performance with non-expert workers in several NLP tasks.
Raykar~\etal~\cite{raykar:nips2011ranking,raykar:icml2009,raykar:jmlr2010}
studied how to learn a model with noisy labeling. 
Specifically, they employ a logistic regression classifier, 
and insert hidden variables indicating whether a worker tells the truth. 
%They utilize an EM algorithm to infer the real ground-truth while training the model.
Karger~\etal~\cite{karger:nips2011} proposed an iterative algorithm to infer workers' reliability 
and aggregating their answers.  

Whitehill~\etal~\cite{whitehill:nips2009} modeled the annotator ability, data item difficulty, and inferred the true label from the crowds in a unified model.
Demeester~\etal~\cite{demeester:wsdm2014} discussed the disagreement between different users on assessment of web search results.
However, they do not study the case when data items are grouped into batches.

Venanzi~\etal~\cite{venanzi:www2014} proposed a community-based label aggregation model to identify different types of workers, 
and correct their labels correspondingly.  
Das~\etal~\cite{das:kdd2013} addressed the interactions of opinions between people connected by networks.
Their studies focus on another aspect of dependencies, 
which is the dependencies between workers, 
while in our studies, we are more concerned about dependencies between data items and their judgments.  


\vpara{Crowdsourced ranking and rating.}
There is related research on leveraging crowds' power to obtain ranking of data items.  
Chen~\etal~\cite{chen:wsdm2013} studied aggregating crowdsourced annotation on pairwise comparison to obtain a ranking on data items. 
They employed the simplified version of Plackett-Luce model for pairwise comparison, Bradley-Terry model~\cite{bradley:1952}, 
and extended the model to adapt the crowdsourcing settings.  
Similar problem is also studied by Volkovs~\etal~\cite{volkovs:www2012}, 
while they proposed a multinomial preference model for partial ranking of data items.  
%and apply the model on aggregating crowdsourced comparison on data item pairs into a ranking.  
However, their objective is just to obtain a ranking, 
while we further need to determine the labels for data items.  
Furthermore, our model can deal with not only pairwise comparisons, 
but also the situation with more than two data items in a batch.  

The technique for aggregating different rankings into a unified ranking has also been studied 
in context of information retrieval~\cite{klementiev:icml2008,liu:www2007,qin:nips2010}.  
A summary of early work can be found in~\cite{dwork:www2001}.
However, most of them do not explicitly address the crowdsourcing settings and might not be directly applicable. 


Several studies also focus on crowdsourced rating.  
Parameswaran~\etal~\cite{parameswaran:vldb2014} focused on crowdsourced rating on items, 
and applied their system on a peer evaluation data set of a MOOC course.  
Crowdsourcing has also been utilized for rating multimedia content quality~\cite{chen:mm2009} and relevance assessment~\cite{alonso:2008}.  
However, their techniques do not provide a straightforward solution to determine a threshold 
when the objective is to obtain labels.  



\vpara{Annotation bias in sequential and batch judgments.}
Scholer~\etal~\cite{scholer:sigir2013,scholer:sigir2011} studied the annotation disagreements in 
a relevance assessment data set.  
%TREC data set, which is on relevance assessment tasks in information retrieval.  
They discovered the correlation between annotations of similar data items.  %, and the estimated time between the two annotation.  
They also explored ``threshold priming'' in annotation, 
where the annotators tend to make similar judgments or apply similar standard on consecutive data items they review. 
%tend to make similar judgments to consecutive data items.  
However, their studies focus on the scenario when data items are organized in a long sequence.  
It confines the dependencies to exist only between consecutive data items.  
Also, their studies focus more on qualitative conclusions, without a quantitative model to characterize and measure the discovered factors.  
Carterette~\etal~\cite{carterette:effect2010} provided several assessor models for the TREC data set.  
Mozer~\etal~\cite{mozer:nips2010} studied the similar ``relativity of
judgments'' phenomenon on sequential tasks instead of small batches.  
Again, their focus is more on data items presented as a long sequence, while we focus more on data items presented in small batches.  
%Also, they focused more on debiasing not active learning.  %but did not apply active learning based on the proposed models.   
%p

Our recent work~\cite{zhuang:wsdm2015} also considered the similar setting when data items are organized in small batches.  
We verified the existence of annotation bias caused by batching data items.  
We also designed an active learning algorithm to smartly assemble batches, 
aiming to improve the performance of classifier trained on this annotation batches.  
However, we did not focus on improving the quality of labels collected, 
and we still used majority voting to obtain labels for data items.  
In this paper, we elaborate this work by debiasing the obtained labels per se, 
and expecting the higher label quality can benefit tasks including training and/or evaluating classifier, 
which potentially can have a broader application.  



\hide{
\vpara{Batch active learning.}
Active learning has been extensively studied.
Settles~\etal~\cite{settles:2010survey} summarized a number of active learning strategies.
Batch active learning, in contrast to traditional active learning settings, aims to choose a set of data items to query, which proposes some unique challenges.
Some strategies focus on optimizing the helpfulness of a data batch.
Hoi~\etal~\cite{hoi:icml2006,hoi:www2006} utilized Fisher information matrix to choose the data items that are likely to change the classifier parameters most.
Brinker~\etal~\cite{brinker:icml2003} studied batch active learning for SVM and aimed to maximize the diversity within the selected set of data samples.
Guo~\etal~\cite{guo:nips2008} proposed discriminative active learning strategy by formulating the problem as an optimization problem.
% Azimi~\etal~\cite{azimi:icml2012} propose a Monte-Carlo method to sequentially select the batch of samples to query.
A number of strategies also aim at choosing the most representative data batch with regard to the unlabeled data set.
Yu~\etal~\cite{yu:icml2006} proposed a transductive experimental design, which prioritizes the data samples that represent the hard-to-predict data.
Chattopadhyay et al.~\cite{chattopadhyay:kdd2012} tried to choose the data batch to minimize Maximum Mean Discrepancy (MMD) to measure the difference in distribution between the labeled and unlabeled data.
There are studies addressing both intuitions.
Wang~\etal~\cite{wang:kdd2013} designed a framework to minimize the upper bound of empirical risk, which aims to find a batch of data items that are both discriminative and representative.
However, all of the above studies assume reliable oracles---which never are in multiple-annotator scenarios such as crowdsourcing.

\vpara{Active learning with crowds.}
Crowdsourcing serves as a potentially ideal oracle for active learning.
Two perspectives have been explored; 
first is how to select data items to query when the oracles are noisy.
Sheng~\etal~\cite{sheng:kdd2008} provided an empirical study on the performance of repeated labeling and developed an active learning strategy addressing both the ``label uncertainty'' and ``model uncertainty''.
Second is how to select annotators for crowdsourcing.
Donmez~\etal~\cite{donmez:kdd2009} studied this problem, by modeling the querying problem as a multi-armed bandit problem.  Each annotator is regarded as as a bandit, and a binary reward function is defined based on whether the oracle provides a correct label.  %However, the true label is unknown. They hence utilize majority voting to determine the ground truth.
Yan~\etal~\cite{yan:icml2011} explore both the problem of selecting query samples and selecting oracles, in context of a logistic regression classifier.
Kajino~\etal~\cite{kajino:aaai2012} proposed a convex optimization function for active learning in crowds.  
% vijayanarasimhan~\etal~\cite{vijayanarasimhan:cvpr2011} applied crowdsourcing to actively learn the detector for p
However, none of them leverages in-batch bias for active learning.
}


