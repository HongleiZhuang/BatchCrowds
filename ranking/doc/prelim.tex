%!TEX root=betamain.tex

%\vspace{0.2in}
\section{Preliminaries}
\label{sec:prelim}
%



\hide{
We start by introducing the background of a crowdsourcing platform, CrowdFlower.
Then we define the notations used in this paper and formalize the research problem we study.


\subsection{Crowdsourcing Platform}

We briefly introduce the background of an online crowdsourcing platform, CrowdFlower\footnote{\url{http://www.crowdflower.com/}}.
On CrowdFlower, users can design and submit a job to the online platform.
The platform will automatically assign the job to global crowd workers (termed as ``contributors'' in CrowdFlower).
Each ``job'' submitted consists of a data set, an instruction of how the data set should be labeled
and the interface designed for workers to interact with the platform.
A data set contains several ``units'', and each unit may be assigned to multiple workers to work on.
Each worker can generate a ``judgment'' for each unit.
Units are the minimum piece of work to be priced and assigned to workers.
However, for some tasks the minimum unit price (\ie~0.01 USD) might be too high,
and it would be extremely inefficient for workers to judge only a data item in a unit,
especially when the judgment needs to be done based on some context that potentially may be shared by different data items.
For example, in the task of identifying whether a document is relevant to a certain query,
if each unit is designed to be a pair of document and query,
a worker has to read both the query and the document to make a judgment;
in contrast, if a unit is designed to be a query and a batch of documents,
the worker can read the query once, and make several judgments on multiple documents within a query.
Thereby workers can save time,
and users can lower their cost of data annotation.


For each job, the user can require all the workers to go through a certain number of ``test questions''
before they actually start working on the job.
Test questions are units of which the correct judgment is already known.
Therefore one can evaluate the accuracy of each worker.
Usually workers with an accuracy lower than 70\% on test questions are not allowed to work on the job.
We believe that these test questions can be better utilized rather than simply calculating the overall accuracy.
Richer information about worker behavior might be hidden within the judgments of test questions.
Users can also insert some units with known answers intermingled with other units while workers work on the job,
to monitor the real time accuracy of each worker.
Similarly, if the accuracy of a worker drops to lower than 70\%, she will be forbidden from working on the rest of the job.

After the judgments from (multiple) workers toward a unit is obtained,
one can aggregate these judgments by a majority voting strategy to determine the final annotation of this unit.
However, this strategy may not be the best strategy due to the possible annotation bias when each unit contains multiple data items.
We can also develop other aggregating strategies to apply on the judgments,
which better recovers the true labels of data items.
}

%

In this section, we formally define the concepts and notations we use in this paper;
we then formalize the problem of debiasing crowdsourced batches.


\subsection{Basic Concepts and Terminology}

First we need to formalize several basic concepts in a crowdsourcing platform.
Suppose we are given a set of data items $X = \{x_i\}$, where $i=1, \ldots, n$.
Each data item is associated with a label $y_i \in \mathcal{Y}$, 
and we thereby define $Y = \{y_i\}_{i=1}^n$.
In following discussion, we focus on a binary classification task, 
where $\mathcal{Y} = \{0, 1\}$, 
but our framework generalizes to multi-class or rating cases seamlessly (Cf. Section~\ref{sec:ext}).  
According to a standard formalization in learning theory for binary classification, 
we suppose each $(x_i, y_i)$ is generated from a joint probability distribution $P_{\mathcal{X} \mathcal{Y}}$.
We define an inherent score $\eta_{x_i}$ to be the conditional probability $P(y_i = 1 | x_i)$.  
For simplicity, we denote the inherent score as $\eta_i$.  

In a job or task submitted to a crowdsourcing platform,
we can assemble several data items into a batch. %  (``unit'' in CrowdFlower terminology).p
Each batch $\bb_j$ is represented by a set of indices of data items in the batch,
denoted as $\{b_{j1}, \ldots, b_{jk}\}$, 
where $k$ is the size of a batch. 
To be strict, data items in the batch should be represented by $\bx_j = \{x_{b_{j1}}, \ldots, x_{b_{jk}}\}$.  
However, for simplicity, we denote data items in the batch specified by $\bb_j$ as $\{x_{j1}, \ldots, x_{jk}\}$.  
Similarly, we define $\by_j = \{y_{j1}, \ldots, y_{jk}\}$ to be true labels associated with data items in $\bx_j$, 
where $y_{jl}$ is the true label of $x_{jl}$ according to $Y$, $\forall 1 \leq l \leq k$.  
In CrowdFlower language, 
a batch corresponds to a single ``unit'', 
% several data items are assembled into a single ``unit'',pp
where a worker has to judge the entire unit at the same time;
in Mechanical Turk language,
a batch corresponds to a single ``HIT'' (short for Human Intelligence Task).
Usually, data items in the same batch might share the same context, background, or the same instruction, 
in order to reduce the overhead.  
For example, if one is asked to judge whether 
a review about a restaurant is positive or negative, 
it might save time for workers  
by grouping reviews of the same restaurant into the same batch, 
as they only need to read the description of the restaurant once 
before they can make multiple judgments on different reviews.  

\hide{
    Notice that we are only concerned with the scenario when data items are formed into \emph{small batches}.
    Depending on different format of data items, a small batch can have different scale of size.
    For example, in inappropriate comments identification task, we basically confine $k \leq 5$.
}

%

As we assemble data items into batches, 
each worker has to judge the entire batch as a single judgment.  
Given a batch $\bb_j$, the judgment provided by a worker can be represented as $\by_j' = \{y_{j1}', \ldots, y_{jk}'\}$, 
where $y_{jl}' \in \mathcal{Y}$ is the annotation of data item corresponding to 
$x_{jl}$, provided by the worker.
Noting that the worker annotation $\by_j'$ can be different from the true label $\by_j$.  
We refer to worker annotation as ``\emph{annotation}'', 
while the ground-truth label is referred to as simply the ``\emph{label}''.  

In CrowdFlower, as a judgment can only be made based on a unit,
workers are not allowed to submit partial results on a batch 
(as with Mechanical Turk).
However, one can always add an ``unknown'' option for every data item,
so that the workers can provide partial results on a batch.  
For simplicity, we consider no partial judgments in the rest of the paper.  
% Our model can also be easily adapted to the scenario with partial judgment on a batch.
%We do not allow users to provide partial feedback as it complicates the pricing system.  

Now, we are in a position to give a formal definition for a batch of data items:
\begin{definition}
[Batch]
Given a data set $(X, Y)$,
a batch of data items with size $k$ extracted from the given data set can be represented as $(\bb_j, \bx_j, \by_j, \by_j')$, 
where $\bb_j = (b_{j1}, \ldots, b_{jk})$ is a set of indices for $X$ and $Y$; 
$\bx_j = \{x_{j1}, \ldots, x_{jk}\}$ is a set of all the data items, indexed by $\bb_j$; % (\ie~$x_{jl} = x_{b_{jl}}$); 
$\by_j = \{y_{j1}, \ldots, y_{jk}\}$ consists of the corresponding true labels of data items in $\bx_j$; %, also indexed by $\bb_j$; 
$\by_j' = \{y_{j1}', \ldots, y_{jk}'\}$ is the worker annotation on the set of the batch.  
\end{definition}

%p
\noindent Additionally, a set of batches can be defined as:

\begin{definition}
Given a data set $(X, Y)$, 
a set of batches extracted from the given data set is denoted as $\cA = (B, X_B, Y_B, Y_B')$, 
where $B=\{\bb_j\}_{j=1}^m$ consists of the indices of each batch; 
$X_B=\{\bx_j\}_{j=1}^m$ is the set of data item batches, 
with their corresponding true labels $Y_B=\{\by_j\}_{j=1}^m$ 
and worker annotations $Y_B'=\{\by_j'\}_{j=1}^m$.  
\end{definition}

\vpara{Remarks.} 
1) Notice that a data item $x_i \in X$ may certainly appear in multiple batches in $\cA$.
That is, $x_{jl}$ and $x_{j'l'}$ may refer to the same data item as long as $b_{jl} = b_{j'l'}$;
% If data items in two different batches share the same 
% index as indicated by corresponding item in $\bb_j$, 
% they refer to the same data item in $X$;  
2) For the sake of fully utilizing the workforce of crowds, 
without loss of generality, we focus on the scenario when all batches have the identical size $k$. 
However, our model generalizes to the case when batches have different sizes;    
3) In some real world crowdsourcing platforms, a batch can actually be judged by multiple workers,
which means there could be multiple $\by_j'$'s associated to a single $(\bb_j, \bx_j, \by_j)$--- for instance, this is referred to as multiple {\em assignments} on Mechanical Turk.  
However, for the purposes of debiasing, 
it is equivalent to regard a single batch as multiple batches with identical $(\bb_j, \bx_j, \by_j)$
but associated with judgments made by different workers $\by_j'$. 

%This does not affect our proposed model.  pp


\hide{
Denote all the batches as a set of batches $B = \{b_j\}$ where $j = 1, \ldots, m$.
Notice that a data item can appear in multiple batches in $B$.
For the sake of fully utilize the workforce of crowds,
without loss of generality,
we only consider the scenario when all the batches in $B$ have the identical size $k$.

We denote all the judgments on a batch set $B$ as $Y'$.
In real world crowdsourcing platform, a batch can actually be judged by multiple workers, 
which means there could be multiply $y_j'$'s associated to a single batch $b_j$.  
However, this is equivalent to regard a single batch $b_j$ as multiple identical batches with different indices, 
and assign each batch with a unique judgment made by different workers.  
This does not affect the effectiveness of our proposed model.  
}


\hide{
Now we formalize the definition of a data set:
\begin{definition}
[Data Set of Batches]
A data set of batches consists of $(X_B, Y_B)$,
where $B = \{b_j\}_{j=1}^m$ is a set of batches with each $b_j = \{b_{j1}, \ldots, b_{jk}\}$ indicating a set of indices;
each batch $x_j \in X_B$ is a set of data items $\{x_{j1}, \ldots, x_{jk}\}$ indexed by $b_j$,
and each batch $y_j \in Y_B$ indicates the corresponding labels $\{y_{j1}, \ldots, y_{jk}\}$.
\end{definition}
}


%We can simply regard them as judgments on multiple identical batches indexed differently, but with the same data items.

\subsection{Problem Definition}

Based on the concepts described thus far, 
we can formalize the problem of debiasing crowdsourced batches as the following:

\begin{problem} [Debiasing Crowdsourced Batches]
Suppose we have a labeled data set $(X_L, Y_L)$ with $Y_L$ known, 
as well as its extracted batches and their crowdsourced annotation $(B_L, X_{B_L}, Y_{B_L}, Y_{B_L}')$.  
If we are then given another unlabeled data set $X_U$, 
as well as its extracted batches and crowdsourced annotation $(B_U, X_{B_U}, Y_{B_U}')$, 
the objective is to infer the true labels $Y_{U}$ associated with $X_{U}$ from the crowdsourced annotation.  
\end{problem}
Notice that our problem formulation as described above requires
as input labeled and annotated data items for training purposes. 
In practice, the labeled data for training 
can be collected from the ``test questions'' 
with ground-truth labels, 
inserted by the crowdsourcing platform for the 
purpose of quality control and monitoring
of workers.  
The usage of test questions is standard practice: 
As an example, in CrowdFlower, all workers have to attempt a 
certain number of test questions with correct labels 
and need to achieve an accuracy over a certain threshold (\eg~70\%) 
before they can proceed to work on the regular task(s).  
Also, additional hidden data items with known labels 
can be inserted into the regular tasks
to monitor the accuracy of workers.  
In our setting, worker behavior on these
test questions or labeled data can additionally be used
for training purposes.


Also notice that in this version of our problem
formulation, we assume identical worker behavior.  
This is a more standard setting in crowdsourcing practice 
as there is usually not enough work done by each worker to ascertain individual behavior.
Also, it is straightforward to extend our model when 
different workers have different behavior when working on tasks.
% \agp{can we say something like this:
% In practice, the version where worker behavior is
% assumed to be identical is also more standard because
% often we don't have enough work done by each worker to
% ascertain individual worker behavior.} 



\hide{
If we do want to model the worker behavior 
This assumption is not always true, 
but in order to reduce the 
but considering the data sparsity in real world crowdsourcing scenario 
(each worker usually answers only 10 batches test questions),
building an accurate enough worker model for each worker is not realistic.
}
%ppp



\begin{table}[!t]
\centering
 {\caption{Notation description.}\label{tab:notation}}
{
  \begin{tabular}{@{}c@{}||p{6.8cm}}
  \hline    
  Notation & Description \\ \hline \hline
  $X$      & Set of all the data items $\{x_i\}_{i=1}^n$  \\ \hline
  $Y$      & Set of all true labels associated with data items in $X$ \\ \hline
  $\bb_j$    & A set of data item indices $\{b_{jl}\}_{l=1}^{k}$ \\ \hline
  \multirow{2}{*}{$\bx_j$}    & A data item batch $\{x_{jl}\}_{l=1}^{k}$ where $x_{jl}$ is extracted from the data item in $X$ with index specified by $b_{jl}$ \\ \hline
  \multirow{2}{*}{$\by_j$}    & A label batch consists of true labels $\{y_{jl}\}_{l=1}^{k}$ associated with data items in $\bx_j$ \\ \hline
  \multirow{2}{*}{$\by_j'$}   & Worker annotation collected from a crowdsourcing platform for data items in $\bx_j$ \\ \hline
  $B$      & Set of all the batches $\{\bb_j\}_{i=1}^m$ \\ \hline
  $X_B$    & Set of all the data item batches \\ \hline
  \multirow{2}{*}{$Y_B$}    & Set of all the true labels associated with data item batches in $X_B$ \\ \hline
  \multirow{2}{*}{$Y_B'$}   & Set of all the worker annotation from crowds on data item batches in $X_B$ \\ \hline
  \end{tabular}
}
%\vspace{-0.3in}
\end{table}
