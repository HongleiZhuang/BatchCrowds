%!TEX root=betamain.tex

\section{Debiasing Annotation}
\label{sec:debias}

In this section, we introduce our method that debiases
annotations collected for small batches of
data given  the trained worker model.
More precisely, given a set of $n_U$ unlabeled data items $X_U$,
assembled into $m_U$ batches represented by $B_U$,
as well as their annotations obtained from the crowds $Y_U'$,
how do we infer their true labels $Y_U$?

The basic idea is, based on the given worker model,
we infer $\eta_i$ for each $x_i \in X_U$.
Then, we 
 simply apply the Bayes classifier to determine the inferred label,
which yields $\hat{y}_i = 1$ if $\eta_i > 0.5$, or $\hat{y}_i = 0$ if $\eta_i \leq 0.5$.

We again adopt a maximum likelihood estimation techique.
The log-likelihood of the obtained annotation is:
\beal{
	\log L (\eta) = \sum_{j=1}^{m_U}
	\log \biggl[
		&\hat{\lambda} %\prod_{t=1}^{k} \eta_{jt}^{y_{jt}} (1 - \eta_{jt})^{(1 - y_{jt})}
		\prod_{x_{jt1} \in X_{j}^1} \eta_{jt1} \prod_{x_{jt0} \in X_{j}^0} (1 - \eta_{jt0}) \nonumber \\
		+ &(1 - \hat{\lambda}) \hat{p}_{\tau_j} P(X_{j}^1 \succ X_{j}^0)
	\biggr]
}%

Notice that $\hat{\lambda}$ and $\hat{p}_{\tau_j}$ are parameters learned from Section~\ref{sec:worker},
and $P(X_{j}^1 \succ X_{j}^0)$ is also a function of $\eta_{i}$'s.
Similarly, \agp{similar to the previous section?} we apply an EM-algorithm here
by first calculating $\hat{\lambda}_j$ for each batch at the E-step 
according to~\eqref{eq:e_step}
but replacing $\lambda$ and $p_{\tau}$ by the value we learned during the training step.
Then we have:
\beal{\label{eq:em_debiasing}
	\log L(\eta) &\geq \sum_{j=1}^{m_U}
		\hat{\lambda}_j \biggl[
			\sum_{x_{jt1} \in X_{j}^1} \log \eta_{jt1}  + \sum_{x_{jt0} \in X_{j}^0} \log (1 - \eta_{jt0})
		\biggr] \nonumber \\
		& + \sum_{j=1}^{m_U} (1 - \hat{\lambda}_j) \bigl[\log \hat{p}_{\tau_j}  + \log P(X_{j}^1 \succ X_{j}^0) \bigr]
}%
where the second term includes $\log P(X_{j}^1 \succ X_{j}^0)$, which is hard to optimize.
We apply the idea of the EM-algorithm again here.
We use notation $R_j$ to represent $R(X_{j}^1, X_{j}^0)$.
For each $\pi \in R_j$, we can calculate its conditional probability given $X_{j}^1 \succ X_{j}^0$, denoted as $\hat{q}_{\pi}$ by:
\beal{
	\hat{q}_{\pi} = P(\pi | X_{j}^1 \succ X_{j}^0; \hat{\eta}) = \frac{P(\pi; \hat{\eta})}{\sum_{\pi \in R_j}P(\pi; \hat{\eta})}
}%
which is the E-step.
According to Jensen's inequality we have:
\beal{\label{eq:semi_ranking_to_sum_of_ranking}
	\log P(X_1 \succ X_0) &= %\log \sum_{\substack{\pi: \forall x_{1} \in X_1, x_0 \in X_0, \\ \pi^{-1}(x_{0}) > \pi^{-1}(x_{1}) }} P(\pi)
	\log \sum_{\pi \in R_j} P(\pi) \nonumber \\
	&\geq \sum_{\pi \in R_j} \hat{q}_{\pi} \log P(\pi)
}%
where the last inequality yields the objective function we want to optimize.
The correctness of EM-algorithm guarantees the convergence of optimizing this function.


\hide{
We further utilzie Jensen's inequality to bound this term from bottom by
\beal{\label{eq:semi_ranking_to_sum_of_ranking}
	\log P(X_1 \succ X_0) &= %\log \sum_{\substack{\pi: \forall x_{1} \in X_1, x_0 \in X_0, \\ \pi^{-1}(x_{0}) > \pi^{-1}(x_{1}) }} P(\pi)
	\log \sum_{\pi \in R_j} P(\pi) \nonumber \\
	&\geq \frac{1}{|R_j|} \sum_{\pi \in R_j}  \log P(\pi)
}%
where
}


Furthermore, according to the minorization-maximization (MM) algorithm used in~\cite{hunter:aos2004},
we obtain the lower bound for $\log P(\pi)$, which is defined by the Plackett-Luce model, by:
\beal{\label{eq:mm_algorithm}
	\log P(\pi) &= \sum_{t=1}^{k-1} \biggl[
		\log \eta_{\pi^{-1}(t)} - \log \sum_{s=t}^{k} \eta_{\pi^{-1}(s)}
	\biggr] \nonumber \\
	&\geq \sum_{t=1}^{k-1} \biggl[
		\log \eta_{\pi^{-1}(t)} - \frac{\sum_{s=t}^{k} \eta_{\pi^{-1}(s)}}{\sum_{s=t}^{k} \hat{\eta}_{\pi^{-1}(s)}}
	\biggr]
}%
where $\hat{\eta}_i$ is the estimated parameter of last iteration.



By combining \eqref{eq:em_debiasing},~\eqref{eq:semi_ranking_to_sum_of_ranking} and \eqref{eq:mm_algorithm},
we obtain the objective function to optimize as:
\beal{\label{eq:objective_function}
	Q(\eta) &= \sum_{j=1}^{m_U}
		\hat{\lambda}_j \biggl[
			\sum_{x_{jt1} \in X_{j}^1} \log \eta_{jt1}  + \sum_{x_{jt0} \in X_{j}^0} \log (1 - \eta_{jt0})
		\biggr] \nonumber \\
		&+ \sum_{j=1}^{m_U} (1 - \hat{\lambda}_j)  \sum_{\pi \in R_j}  \hat{q}_{\pi}
		\sum_{t=1}^{k-1} \biggl[
			\log \eta_{\pi^{-1}(t)} - \frac{\sum_{s=t}^{k} \eta_{\pi^{-1}(s)}}{\sum_{s=t}^{k} \hat{\eta}_{\pi^{-1}(s)}}
		\biggr]
}%
Notice that $Q(\eta)$ is actually a lower-bound of the original log-likelihood function.
Moreover, for two EM-step and one MM-step we apply in deriving the $Q$-function,
it is proven that by improving $Q(\eta)$ from this iteration $Q(\hat{\eta})$,
we can achieve no less improvement on the log-likelihood. \agp{I don't understand the previous sentence}
Therefore optimizing $Q(\eta)$ can also optimize the log-likelihood.



Take the derivative, we obtain
\beal{\label{eq:derivative_of_objective_function}
    \frac{\partial Q(\eta)}{\partial \eta_i} &=
        \sum_{j \in M_1(i)}\frac{1}{\eta_i}
        - \sum_{j' \in M_0(i)} \frac{ \hat{\lambda}_{j'}}{1 - \eta_i} \nonumber \\
        %&+ \sum_{j \in M_1(i)} \frac{1 - \hat{\lambda}_j}{\eta_i}
        &- \sum_{j=1}^{m_U} (1 - \hat{\lambda}_j) \sum_{\pi \in R_j} \hat{q}_{\pi} \biggl[
            \sum_{t=1}^{|X_j^1|} \frac{\mathbf{1}_{\{\pi^{-1}(i) \geq t \}}}{
                \sum_{s=t}^{k} \hat{\eta}_{\pi^{-1}(s)}
            }
        \biggr]
}%
where $M_1(i)$ and $M_0(i)$ are defined as $M_y(i) = \{j:x_i \in X_j^y\}$ for $y \in \{0, 1\}$.
The updating rule can be obtained by solving $\partial Q(\eta) / \partial \eta_i = 0$, namely
\beal{\label{eq:update_eta}
	\hat{\eta}_i = \frac{T_1 + T_2 + T_3 - \sqrt{(T_1 + T_2 + T_3)^2 - 4 T_1 T_3}}{2 T_3} 
}
where
\beal{
	T_1 &= |M_1(i)|,~~~~~~T_2 = \sum_{j' \in M_0(i)} \hat{\lambda}_{j'} \nonumber \\
	T_3 &= \sum_{j=1}^{m_U} (1 - \hat{\lambda}_j) \sum_{\pi \in R_j} \hat{q}_{\pi} \biggl[
	            \sum_{t=1}^{|X_j^1|} \frac{\mathbf{1}_{\{\pi^{-1}(i) \geq t \}}}{
	                \sum_{s=t}^{k} \hat{\eta}_{\pi^{-1}(s)} \nonumber
	            }
            \biggr]
}

By iteratively updating the scores to optimize the likelihood of the annotation on test data,
we can obtain the inferred predictive distribution of each item.
Based on this, we can determine the inferred binary label for each data item by assigning $y_i'=1$ if $\hat{\eta}_i > 0.5$,
or $y_i'=0$ otherwise.
Notice that we do not further tune the threshold in this step,
as the scores we learned here are expected to be a reasonable estimate of the true $\eta_i$'s.
Therefore, if the predictive distribution \agp{fix if you changed it before} are known,
learning theory guarantees us that by using Bayes classifier (namely to take 0.5 as threshold)
is supposed to yield the best expected performance. \agp{what is supposed to yeild? rephrase sentecne}




\incmargin{1em}
\begin{algorithm}[!t]
       \caption{Debiasing crowdsourced annotation on batches of data items.}\label{alg:debias}
\small
       \KwIn{Data batches $B_U = \{b_j\}$, crowdsourced annotation $Y_U'=\{y_j'\}$;
             training data batches $B_L$, crowdsourced annotation $Y_L'$, true labels $Y_L$.}
       \KwOut{Inferred labels $Y_U$.}

       \BlankLine
       \tcp{Training work model;}
       $\hat{\lambda}_j \leftarrow 0.5$;
       Initialize $\hat{p}_{\tau}$ by random values; \\
       \Repeat{$L$ converged}
       {
           Update $\hat{\lambda}_j$ bfor $\forall 1 \leq j \leq m_L$ by~\eqref{eq:e_step}; \\
           Update $\hat{\lambda}$ and $\hat{p}_{\tau}$ by~\eqref{eq:m_step}
       }
       \BlankLine
       \tcp{Calculate debiased labels;}
       \Repeat{$Q(\eta)$ converged}
       {
           Update $\eta_i$ for $\forall 1 \leq i \leq n_U$ by~\eqref{eq:update_eta};
       }
       $y_i \leftarrow \mathbf{1}_{\{\eta_i > 0.5\}}$ for $\forall 1 \leq i \leq n_U$; \\
       Output $Y_U$.
\end{algorithm}
\normalsize
\decmargin{1em}


