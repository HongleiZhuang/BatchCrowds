\begin{abstract}

Crowdsourcing is the de-facto standard for gathering annotated data.
While, in theory,
data annotation tasks are assumed to be attempted by workers independently,
in practice,
data annotation tasks are often grouped into batches to be presented
and annotated by workers together,
in order to save on the time or cost overhead of providing instructions or necessary background.  
Thus, even though independence is usually assumed between annotations on data items within the same batch,
in most cases, a worker's judgment on a data item can still be affected by other data items within the batch, leading to additional errors in collected labels.
%Although annotation bias of individual data items and sequential data items has been explored,
%little research has been done on batches of data items.
%Grouping data items in batches can facilitate crowdsourced data annotation of a great number of tasks
% (\eg~clustering, outlier detection~\etc).
In this paper, we study the data annotation bias when data items are presented as batches
to be judged by workers simultaneously.
We propose a novel worker model to characterize the annotating behavior on data batches,
and present how to train the worker model on annotation data sets.
We also present a debiasing technique to remove the effect of such annotation bias
from adversely affecting the accuracy of labels obtained.
Our experimental results on synthetic and real-world data sets
demonstrate that our proposed method can achieve up to +57\% improvement in $F_1$-score 
compared to the standard majority voting baseline.  
%and up to +14-17\% improvement comparing to a tuned majority voting strategy.  

%the effectiveness of our proposed method.

\hide{
Data annotation bias is found in many situations.  Often it can be ignored
as just another component of the noise floor.  However, it is especially
prevalent in crowdsourcing tasks and must be actively managed.  Annotation
bias on single data items
has been studied with regard to data difficulty, annotator bias, etc., while
annotation bias on batches of multiple data items simultaneously presented to
annotators has not been studied.  In this paper, we verify the existence of
``in-batch annotation bias'' between data items
in the same batch.  We propose a factor graph based batch annotation model to
quantitatively capture the in-batch annotation bias, and measure the bias during a
crowdsourcing annotation process of inappropriate comments in LinkedIn.  We
discover that annotators tend to make polarized annotations for the entire batch
of data items in our task.  We further leverage the batch annotation model to propose a
novel batch active learning algorithm.  We test the algorithm on a real
crowdsourcing platform and find that it outperforms in-batch bias na\"{\i}ve
algorithms.
}

\end{abstract}
