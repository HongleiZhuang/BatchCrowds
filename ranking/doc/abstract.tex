\begin{abstract}

Data annotation bias is prevalent in crowd annotated data.  

\hide{
Data annotation bias is found in many situations.  Often it can be ignored
as just another component of the noise floor.  However, it is especially
prevalent in crowdsourcing tasks and must be actively managed.  Annotation 
bias on single data items
has been studied with regard to data difficulty, annotator bias, etc., while
annotation bias on batches of multiple data items simultaneously presented to
annotators has not been studied.  In this paper, we verify the existence of
``in-batch annotation bias'' between data items 
in the same batch.  We propose a factor graph based batch annotation model to 
quantitatively capture the in-batch annotation bias, and measure the bias during a
crowdsourcing annotation process of inappropriate comments in LinkedIn.  We
discover that annotators tend to make polarized annotations for the entire batch
of data items in our task.  We further leverage the batch annotation model to propose a
novel batch active learning algorithm.  We test the algorithm on a real
crowdsourcing platform and find that it outperforms in-batch bias na\"{\i}ve
algorithms.
}

\end{abstract}
